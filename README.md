# AI-for-Landscape

## File description
- `DataGeneration.py` contains training-data generation through amplifying initial questions and answering to them via Cohere API
- `conversation.py` is the original ouput from running `DataGeneration.py`
- `conversation2.py` is the inspected version of `conversation.py` which removed conversations that seemed to cause inaccuracy
- `LLM.py` contains the architecture and training of the main language model 
- `test.py` contains testing attempts of the language model from `LLM.py` 
- `ScapeGPT.py` contains the final system for this project 

## System details
The system included in this image classificaiton of landscape composition, onject detection, and the main language model about landscape. 

* ### What it does
You insert a landscape image link. The model understands image and ouputs a response about how to draw the landscape.

* ### How it works
  1. The model recognizes the composition found in a landscape image via custom trained image classification model
  2. The main objects found in a landscape image is detected by YOLOv8 model from ultralytics 
  3. The information gained above is converted into a question text and passed through the langauge model to get the response 

## Model Details
The classification model is buiilt with a transfer learning of resnet50 pretrained model. The last dense layer is removed and the new  dense layer inserted to train with a custom dataset for landscape composition. 
[Go to Kaggle page for the full process](https://www.kaggle.com/code/keisukenakamura54/composition-classificaiton-modified)

The main large language model is built with karas sequential model consisting of Embedding, two LSTM, and Dense layers. The first layer is an embedding layer that takes the input sequence and maps each word to a 32-dimensional vector. The second and third layers are LSTM (Long Short-Term Memory) layers that models long-term dependencies in a sequence by selectively retaining from previous time steps. The fourth layer is a dense layer that outputs a probability distribution over the vocabulary of the model.

The idea of this sequantial model was inspired by the [work by Franz Geffke](https://f-a.nz/dev/develop-your-own-llm-like-chatgpt-with-tensorflow-and-keras/).

The input sequence is a collection of lists that contains numbers that each corresponds to each character from one index in a text to a certain distance. the The ouput sequence is a collection of numbers that each corresponds to the next character to each input data, as shown below.
```
"Apple" --> input = [Ap, pp, pl], output = [p, l, e]
input_sequence = [[2, 1], [1, 1], [1, 3]], output_sequence = [1, 3, 4]
```
Each character is converted into sequence using tokenizer. 

## Data sources used for training
As shown in 'DataGeneration.py' file, the training data was generated by first amplifying the initial prompts a certain number times by passing them to `command-nightly` model from Cohere using API, and then answering all the original and amplified questions by passing them to `command-nightly` model from Cohere using API again. After the conversations were generated, I roughly inspected the file and removed the conversations that seemed to be irrelevent or cause inaccuracy. 

## Design process + Stages of development
The initial idea was to train the language model with a single conversation and train with new conversations on top of the previous model. However, all the trials seemed to get trained only for the last datum/conversation, or the training time was not logical for the accuray achieved. Hence I merged the sequences for all the data/conversations to one arrary to train the model in a single time. 

Setting random seed when training the model to make it reproducable was not the right idea. It made the model respond the same thing no matter what the prompts is, so the seed blocked randomness in the model's response too. 

## Model performance
The language model trained specific for landscape returns complete sentences in most trials, but it can sometimes be a broken repitiion of words. Also, although the sentences are compelte, the content of the responses are far away from the exact answer, as shown below.
```
Prompt: 'How can I draw Diagonal composition landscape with 3 trees, 1 car?'
Response: 'er to create a sense of depth and make them more permanent.
3. add details to the landscape. this can be blended with a light touch
- add details such as a shoreline, trees, and buildings. next, draw a line straight across.
connect will help to add color to the landscape. this can be blended with a light touch
- add details such as a shoreline, trees, and buildings. next, draw a line straight acro'
```
This is mainly beacause of the lack of training data, but it seems that it's not always the case. When the model was trained with more dataset, the accuracy did not improve. The ouput began with more reasonable starting words but the following characters/words became worse, such as repeating the same words infinitely. 

In addition, when generating a response, the model does not recognize when to complete the sentence from a given number of characters to generate. This causes the response to be cut in the middle, start generating unnecessary content, or repeating the same words. 

## Future work
* Exploring the tradeoff between the logical starting words and the accuracy of the model when changing the number of training data
* Exploring different model architecture
* Exploring how to complete sentence with a given number of character to generate
* Exploring image (re)generation